#Steps to create a nueral network model for beginners.
#Step1 : Create a simple dataset.
#Step2 : Create a nueral network model
#Step3 : Fit the data from dataset into the created model.
#Step4 : Run the model and reduce the error till desired accuracy is obtained.

#Step 1 : Creating a dataset in which the

#An experiemental drug was tested on individuals from ages 13 to 100 in a clinical trial.
#The trial had 2100 participants. Half were under 65 years old, half were 65 years or older.
#95% of patients 65 or older experienced side effects.
#95% of patients under 65 experienced no side effects.

#Library - We need to import certain libraries like numpy,random and most powerful of all sklearn(sci-kit) from where we can use required functions needed in creating a dataset.

#Functions -
#Randint creates random numbers
#Shuffle function shuffles the these random numbers generated to avoid repetition.
#MinmaxScaler is used for scaling which means the random numbers generated has to be converted to number between 0 to 1 like 0.002,1.0,0.35 etc.


import numpy as np
from random import randint
from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler

#Creating training samples to train our model is done by creating an empty array and for representing them by labels 0/1.
train_labels= []
train_samples= []

Using FOR loop to generate random numbers for training samples.
 for i in range(50):
  #5% of younger generation who did experience side effects
  random_younger = randint(13,64)
  train_samples.append(random_younger)
  train_labels.append(1)
   
   #5% of older generation who did experience side effects
  random_older = randint(65,100)
  train_samples.append(random_older)
  train_labels.append(0)

for i in range(1000):
  #95% of younger generation who didnt experience side effects
  random_younger = randint(13,64)
  train_samples.append(random_younger)
  train_labels.append(0)

  
  #95% of older generation who didnt experience side effects 
  random_older = randint(65,100)
  train_samples.append(random_older)
  train_labels.append(1)
# for looop to see training labels created using the dataset
for i in train_labels:
  print(i)

# for looop to see training samples created using the dataset
for i in train_samples:
  print (i)


#Creating numpyarrays in which the we insert train_labels and train_samples
train_labels = np.array(train_labels)
train_samples = np.array(train_samples)

#using shuffle function we shuffle the values to avoid repetations of numbers or imposed order.
train_labels , train_samples = shuffle(train_labels,train_samples)


#Using MinMaxScaler function to scale the data generated using random function
scaler = MinMaxScaler(feature_range=(0,1))

# Using fit_transform function to transfrom the data from those numbers like 33,64 etc to .33,1.0 etc 
scaler_trained_samples = scaler.fit_transform(train_samples.reshape(-1,1))


#you can see the transformation by running this loop.
for i in scaler_trained_samples:
  print(i)

#Creating nueral network model ,Using keras in tensorflow
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation,Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import categorical_crossentropy 


#Code for checking availablity of gpu available as machine learning requires gpu which is freely available in googlecolab
physical_devices = tf.config.experimental.list_physical_devices('GPU')
print("NO OF GPUS AVABLE:", len(physical_devices))
tf.config.experimental.set_memory_growth(physical_devices[0], True)


model = Sequential([
                    Dense(units=16,input_shape= (1,), activation='relu'),
                    Dense(units=32,activation='relu'),
                    Dense(units=2,activation='softmax')
])

model.summary()
model.compile(optimizer=)
